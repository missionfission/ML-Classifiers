{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bayes Classifier (with different class conditional densities and estimation techniques)\n",
    "2. Naive Bayes Classifier\n",
    "3. K-means Clustering\n",
    "4. K-Nearest Neighbor Classifier\n",
    "5. Principal component analysis (where ever applicable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Bayes Methods :\n",
    " 1. MLE\n",
    " 2. Bayesian Parameter Estimation\n",
    " 3. Expectation Maximization\n",
    " \n",
    "## Densities to implement :\n",
    "1. GMM\n",
    "2. Gaussian\n",
    "3. Bernoulli \n",
    "4. Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mutlivariate_normal():\n",
    "    def __init__(self,mean,cov):\n",
    "        self.mean=mean\n",
    "        self.cov=cov\n",
    "    def pdf(self,x):\n",
    "        return  (1/(np.power(2* np.pi,mean.shape[0]/2)*np.power(np.linalg.det(cov),0.5))) *(np.exp (-0.5 * (np.linalg.multi_dot([(x - self.mean) , (np.linalg.inv(self.cov)), (x - self.cov).T]))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class bayes:\n",
    "#         def gaussian(self,train,y,n_c):\n",
    "#         def binomial(self,train,y,n_c):\n",
    "#         def multinomial(self,train,y,n_c):\n",
    "#         def mle():\n",
    "#         def gmm(self,train,y,n_c):\n",
    "# from scipy.stats import multivariate_normal         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussian with MLE Estimate    \n",
    "class BayesClassifier:\n",
    "   \n",
    "    mu = None\n",
    "    cov = None\n",
    "    n_classes = None\n",
    "    \n",
    "    def __init__(self):\n",
    "        a = None\n",
    "    \n",
    "    def pred(self,test):\n",
    "        prob_vect = np.zeros((self.n_classes,test.shape[0]))\n",
    "        for i in range(self.n_classes):        \n",
    "            class_cond = multivariate_normal(mean=self.mean[i,:], cov=self.cov[i,:])\n",
    "            prior=self.prior[i]         \n",
    "            prob_vect[i,:] = prior*class_cond.pdf(test)       \n",
    "        return np.argmax(prob_vect,axis=1)\n",
    "        \n",
    "    def fit(self, train,y):\n",
    "        self.mean = []\n",
    "        self.cov = []\n",
    "        self.n_classes = np.max(y)+1\n",
    "        self.prior = np.zeros(self.n_classes) \n",
    "        for i in range(self.n_classes):\n",
    "            tr_c  = train[y==i]\n",
    "            self.prior[i]=tr_c.shape[0]/train.shape[0]\n",
    "            mean_c = np.mean(tr_c, axis=0)\n",
    "            cov_c=np.cov(tr_c.T)\n",
    "            self.mean.append(mean_c)\n",
    "            self.cov.append(cov_c)\n",
    "        self.mean = np.asarray(self.mean)\n",
    "        self.cov = np.asarray(self.cov)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of GMM with EM Algorithm\n",
    "class GMM(BayesClassifier):\n",
    "    def __init__(self,mode):\n",
    "    self.mode=mode\n",
    "\n",
    "    z = np.identity(k)\n",
    "    w = np.tile(z,(int)(Y.size/k) + 1).T\n",
    "    w = w[0:Y.size] \n",
    "\n",
    "    p = np.mean(w,axis = 0)\n",
    "    mean = []\n",
    "\n",
    "    for i in range(k):\n",
    "        mean.append(np.mean(X*np.reshape(w.T[i],(Y.size,1)), axis = 0)/(p[i]))\n",
    "    mean = np.asarray(mean, dtype = float)\n",
    "\n",
    "    variance = []\n",
    "    for i in range(k):\n",
    "        temp = 0\n",
    "        for j in range(Y.size):\n",
    "            temp += w[j][i]*(np.asmatrix(X[i])).T*np.asmatrix(X[i])\n",
    "        variance.append(temp/(Y.size*p[i]))\n",
    "    variance = np.asarray(variance, dtype = float)\n",
    "\n",
    "\n",
    "    w_dash = class_conditionals(X,mean,variance)*p\n",
    "    w_dash = w_dash/np.reshape(np.sum(w_dash,axis = 1),(Y.size,1))\n",
    "\n",
    "    iterations = 0\n",
    "    tolerance = 0.00001\n",
    "\n",
    "    while np.linalg.norm(w_dash - w)>tolerance and iterations<1000 :\n",
    "        w = w_dash\n",
    "\n",
    "        p = np.mean(w,axis = 0)\n",
    "        mean = []\n",
    "        for i in range(k):\n",
    "            mean.append(np.sum(X*np.reshape(w.T[i],(Y.size,1)))/(Y.size*p[i]))\n",
    "        mean = np.asarray(mean)\n",
    "        variance = []\n",
    "        for i in range(k):\n",
    "            temp = 0\n",
    "            for j in range(Y.size):\n",
    "                temp += w[j][i]*(np.asmatrix(X[i])).T*np.asmatrix(X[i])\n",
    "            variance.append(temp/(Y.size*p[i]))\n",
    "        variance = np.asarray(variance,dtype = float)\n",
    "\n",
    "        w_dash = class_conditionals(X,mean,variance)*p\n",
    "        w_dash = w_dash/np.reshape(np.sum(w_dash,axis = 1),(Y.size,1))\n",
    "        w = w_dash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class naivebayes:\n",
    "    \n",
    "    class MultinomialNB:\n",
    "    total_counts = 0\n",
    "    label_counts = {}\n",
    "\n",
    "    feature_counts_per_label = {}\n",
    "    total_features_per_label = {}\n",
    "\n",
    "    text_data = False\n",
    "\n",
    "    # Y has to be a numpy array\n",
    "    def train(self, X, Y, text_data=False):\n",
    "        self.text_data = text_data\n",
    "        self.total_counts = len(X)\n",
    "\n",
    "        Y_unique_counts = np.unique(Y)\n",
    "        Y_bins = np.bincount(Y)\n",
    "        for i, y in enumerate(Y_unique_counts):\n",
    "            self.label_counts[i] = Y_bins[i]\n",
    "            self.feature_counts_per_label[i] = {}\n",
    "            self.total_features_per_label[i] = 0\n",
    "        \n",
    "        \n",
    "        if text_data == False:\n",
    "            for i in range(len(X)):\n",
    "                for feat_i, n_feat in enumerate(X[i]):\n",
    "                    self.total_features_per_label[Y[i]] += n_feat\n",
    "                    if feat_i in self.feature_counts_per_label[Y[i]]:\n",
    "                        self.feature_counts_per_label[Y[i]][feat_i] += n_feat\n",
    "                    else:\n",
    "                        self.feature_counts_per_label[Y[i]][feat_i] = n_feat\n",
    "        else:\n",
    "            for i in range(len(X)):\n",
    "                words = self.format_data(X[i])\n",
    "                self.total_features_per_label[Y[i]] += len(words)\n",
    "                for w in words:\n",
    "                    if w not in self.feature_counts_per_label[Y[i]]:\n",
    "                        self.feature_counts_per_label[Y[i]][w] = 1\n",
    "                    else:\n",
    "                        self.feature_counts_per_label[Y[i]][w] += 1\n",
    "                    \n",
    "\n",
    "    # if text_data=False, features has to be a numpy array\n",
    "    def predict(self, features, smoothing=1, binomial=False):\n",
    "        best_label = 0\n",
    "        best_prob = 0\n",
    "\n",
    "        features = self.format_data(features)\n",
    "        \n",
    "        for label in self.label_counts:\n",
    "            # P(label|features) = P(features|label) * P(label) / P(features)\n",
    "            prob = self.prob_features_given_label(features, label, smoothing=smoothing, binomial=binomial) * self.prob_label(label) / (self.prob_features(features, smoothing, binomial) + 1e-160)\n",
    "            if prob > best_prob:\n",
    "                best_prob = prob\n",
    "                best_label = label\n",
    "            #print(\"Label\", label, \"Prob. feat given label\", self.prob_features_given_label(features, label, smoothing=smoothing, binomial=binomial) * self.prob_label(label))\n",
    "            #print(\"Prob label\", label, \":\", self.prob_label(label))\n",
    "        \n",
    "        #print(\"Labelcounts:\", self.label_counts, \"Totalcounts:\", self.total_counts)\n",
    "        return best_label\n",
    "    \n",
    "    def prob_features_given_label(self, features, label, smoothing=1, binomial=False):\n",
    "        prob = 1\n",
    "        unique_words = []\n",
    "\n",
    "        for i, feat in enumerate(features):\n",
    "\n",
    "            if binomial:\n",
    "                \n",
    "                if self.text_data:\n",
    "                    # binomial probability of a word is of course counted once\n",
    "                    if feat in unique_words: continue\n",
    "\n",
    "                    if feat in self.feature_counts_per_label[label]:\n",
    "                        bernoulli_prob = (self.feature_counts_per_label[label][feat] + smoothing) / (self.total_features_per_label[label])\n",
    "                        prob *= binom.pmf(features.count(feat), len(features), bernoulli_prob)\n",
    "                    else:\n",
    "                        bernoulli_prob = smoothing / (self.total_features_per_label[label] + smoothing*self.label_counts[label])\n",
    "                        prob *= binom.pmf(features.count(feat), len(features), bernoulli_prob)\n",
    "                    \n",
    "                    unique_words.append(feat)\n",
    "\n",
    "                else:\n",
    "                    if i in self.feature_counts_per_label[label]:\n",
    "                        bernoulli_prob = (self.feature_counts_per_label[label][i] + smoothing) / (self.total_features_per_label[label] + smoothing*self.label_counts[label])\n",
    "                        prob *= binom.pmf(feat, features.sum(), bernoulli_prob)\n",
    "                    else:\n",
    "                        bernoulli_prob = smoothing / (self.total_features_per_label[label] + smoothing*self.label_counts[label])\n",
    "                        prob *= binom.pmf(feat, features.sum(), bernoulli_prob)\n",
    "            \n",
    "            else: # bernoulli probability\n",
    "\n",
    "                if self.text_data:\n",
    "                    if feat in self.feature_counts_per_label[label]:\n",
    "                        prob *= (self.feature_counts_per_label[label][feat] + smoothing) / (self.total_features_per_label[label] + smoothing*self.label_counts[label])\n",
    "                    else:\n",
    "                        prob *= smoothing / (self.total_features_per_label[label] + smoothing*self.label_counts[label])\n",
    "                else:\n",
    "                    if i in self.feature_counts_per_label[label]:\n",
    "                        prob *= (self.feature_counts_per_label[label][i] + smoothing) / (self.total_features_per_label[label] + smoothing*self.label_counts[label])\n",
    "                    else:\n",
    "                        prob *= smoothing / (self.total_features_per_label[label] + smoothing*self.label_counts[label])\n",
    "\n",
    "        return prob\n",
    "\n",
    "    \n",
    "    def prob_label(self, label):\n",
    "        return self.label_counts[label] / self.total_counts\n",
    "\n",
    "    def prob_features(self, features, smoothing=1, binomial=False):\n",
    "        prob = 0\n",
    "        for label in self.label_counts:\n",
    "            prob += self.prob_features_given_label(features, label, smoothing=smoothing, binomial=binomial) * self.prob_label(label)\n",
    "        return prob\n",
    "\n",
    "    @staticmethod\n",
    "    def format_data(text):\n",
    "        words = nltk.word_tokenize(text)\n",
    "        return [w.lower() for w in words if w != ',' and w != '.']\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(cov_c2, cov_c3, c2_mean, c3_mean):\n",
    "    z3 = []\n",
    "    z4 = []\n",
    "    x = np.linspace(-4,4,90)\n",
    "    y = np.linspace(-8,8,90)\n",
    "    X, Y = np.meshgrid(x,y)\n",
    "    z = bivariate_normal(X,Y, cov_c2[0][0] ** 0.5, cov_c2[1][1] ** 0.5, c2_mean[0], c2_mean[1], cov_c2[0][1] )\n",
    "    z1 = bivariate_normal(X,Y, cov_c3[0][0] ** 0.5, cov_c3[1][1] ** 0.5, c3_mean[0], c3_mean[1], cov_c3[0][1] )\n",
    "    plt.contour(X, Y, z1-z, colors='blue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self,n_components,*args):\n",
    "        self.n=n_components\n",
    "    def fit_transform(self,train):\n",
    "        U, S, V = self.fit(X)\n",
    "        U = U[:, :self.n]\n",
    "        U *= S[:self.n]\n",
    "        return U\n",
    "    def fit(self,train):\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        X -= self.mean\n",
    "        U,S,V=np.linalg.svd(X,full_matrices=False)\n",
    "        self.components = V\n",
    "        return U,S,V\n",
    "    def transform(self,test):\n",
    "        output = np.dot(test, self.components.T)\n",
    "        return output\n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get EigenVectors\n",
    "covMatrix = numpy.cov(imageList, rowvar=True)\n",
    "eigenVal, eigenVec = numpy.linalg.eig(covMatrix)\n",
    "# Make sure Eigenvectors are unit vectors\n",
    "numpy.testing.assert_array_almost_equal(1.0, numpy.linalg.norm(eigenVec[0]))\n",
    "# Reduce dimension thru eigen vector and save the resulting image\n",
    "largestEigenVec = eigenVec[0]\n",
    "result = largestEigenVec.T.dot(imageList)\n",
    "numpy.uint8(result.reshape(50,66)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self,k,*args)\n",
    "    self.k=k\n",
    "    def predict_one(self,train, y,test):\n",
    "        distances = []\n",
    "        for i in range(len(train)):\n",
    "            distance = ((train[i, :] - test)**2).sum()\n",
    "            distances.append([distance, i])\n",
    "        distances = sorted(distances)\n",
    "        targets = []\n",
    "        for i in range(self.k):\n",
    "            index_of_training_data = distances[i][1]\n",
    "            targets.append(y[index_of_training_data])\n",
    "        return Counter(targets).most_common(1)[0][0]\n",
    "\n",
    "    def result(self,train, y, test):\n",
    "        predictions = []\n",
    "        for x_test in test:\n",
    "            predictions.append(predict_one(train, y, x_test))\n",
    "        return predictions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian(a, b):\n",
    "    return np.linalg.norm(a-b)\n",
    "class k_means:\n",
    "    def __init__(self,k)\n",
    "        self.k=k\n",
    "    def fit(self,dataset,epsilon=0,*args):\n",
    "        history_centroids = []\n",
    "        dist_method = 'euclidian'\n",
    "        num_instances, num_features = train.shape\n",
    "        #define k centroids (how many clusters do we want to find?) chosen randomly \n",
    "        initial = train[np.random.randint(0, num_instances - 1, size=self.k)]\n",
    "        #set these to our list of past centroid (to show progress over time)\n",
    "        history_centroids.append(initial)\n",
    "        #to keep track of centroid at every iteration\n",
    "        prototypes_old = np.zeros(initial.shape)\n",
    "        #to store clusters\n",
    "        belongs_to = np.zeros((num_instances, 1))\n",
    "        norm = dist_method(initial, prototypes_old)\n",
    "        iteration = 0\n",
    "        while norm > epsilon:\n",
    "            iteration += 1\n",
    "            norm = dist_method(prototypes, prototypes_old)\n",
    "            #for each instance in the dataset\n",
    "            for index_instance, instance in enumrate(train):\n",
    "                #define a distance vector of size k\n",
    "                dist_vec = np.zeros((k,1))\n",
    "                #for each centroid\n",
    "                for index_prototype, prototype in enumerate(initial):\n",
    "                    #compute the distance between x and centroid\n",
    "                    dist_vec[index_prototype] = dist_method(prototype, instance)\n",
    "                #find the smallest distance, assign that distance to a cluster\n",
    "                belongs_to[index_instance, 0] = np.argmin(dist_vec)\n",
    "\n",
    "            tmp_prototypes = np.zeros((k, num_features))\n",
    "\n",
    "            #for each cluster (k of them)\n",
    "            for index in range(len(prototypes)):\n",
    "                #get all the points assigned to a cluster\n",
    "                instances_close = [i for i in range(len(belongs_to)) if belongs_to[i] == index]\n",
    "                #find the mean of those points, this is our new centroid\n",
    "                prototype = np.mean(dataset[instances_close], axis=0)\n",
    "                #add our new centroid to our new temporary list\n",
    "                tmp_prototypes[index, :] = prototype\n",
    "\n",
    "            #set the new list to the current list\n",
    "            prototypes = tmp_prototypes\n",
    "\n",
    "            #add our calculated centroids to our history for plotting\n",
    "            history_centroids.append(tmp_prototypes)\n",
    "\n",
    "        return prototypes, history_centroids, belongs_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(dataset, history_centroids, belongs_to):\n",
    "    #we'll have 2 colors for each centroid cluster\n",
    "    colors = ['r', 'g']\n",
    "\n",
    "    #split our graph by its axis and actual plot\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    #for each point in our dataset\n",
    "    for index in range(dataset.shape[0]):\n",
    "        #get all the points assigned to a cluster\n",
    "        instances_close = [i for i in range(len(belongs_to)) if belongs_to[i] == index]\n",
    "        #assign each datapoint in that cluster a color and plot it\n",
    "        for instance_index in instances_close:\n",
    "            ax.plot(dataset[instance_index][0], dataset[instance_index][1], (colors[index] + 'o'))\n",
    "\n",
    "    #lets also log the history of centroids calculated via training\n",
    "    history_points = []\n",
    "    #for each centroid ever calculated\n",
    "    for index, centroids in enumerate(history_centroids):\n",
    "        #print them all out\n",
    "        for inner, item in enumerate(centroids):\n",
    "            if index == 0:\n",
    "                history_points.append(ax.plot(item[0], item[1], 'bo')[0])\n",
    "            else:\n",
    "                history_points[inner].set_data(item[0], item[1])\n",
    "                print(\"centroids {} {}\".format(index, item))\n",
    "\n",
    "                plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
